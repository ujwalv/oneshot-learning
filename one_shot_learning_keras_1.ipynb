{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import numpy.random as rng\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, Input\n",
    "from keras.models import Model\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from matplotlib.pyplot import imread\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Loading Train and Validation data from temp folder \n",
    "train_folder = \"D:\\Dataset\\One_Shot_learning\\omniglot-master\\ExtractedData\\images_background\"\n",
    "val_folder = 'D:\\Dataset\\One_Shot_learning\\omniglot-master\\ExtractedData\\images_evaluation'\n",
    "save_path = 'D:\\MLworkspace\\OneShotLearning\\omniglot_1\\data'\n",
    "model_path = 'D:\\MLworkspace\\OneShotLearning\\omniglot_1\\data'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to structure of the dataset folders, \n",
    "it's necessary to parse every folder and copy the characters(Image itself) into X\n",
    "and the label for these images is the title of each image itself which also contains\n",
    "total number of sample of that image \n",
    "* The Structure of our dataset is as follows Folder->Alphabets->Characters->20 samples of each image\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def loading_data(path,n = 0):\n",
    "    X=[]\n",
    "    y = []\n",
    "    category_dict = {}\n",
    "    language_dict = {}\n",
    "    curr_y = n\n",
    "    # Iterate through all the Alphabets folder and load the names into language_dict\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(alphabet)\n",
    "        language_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        # Iterate through all the folders in Each Alphabet and load the letter into category_dict\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            category_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            # Load all the Images (20) and then the label is added as part of language_dict\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "                # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            language_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X,y,language_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Alphabet_of_the_Magi\n",
      "Anglo-Saxon_Futhorc\n",
      "Arcadian\n",
      "Armenian\n",
      "Asomtavruli_(Georgian)\n",
      "Balinese\n",
      "Bengali\n",
      "Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "Braille\n",
      "Burmese_(Myanmar)\n",
      "Cyrillic\n",
      "Early_Aramaic\nFuturama\n",
      "Grantha\nGreek\n",
      "Gujarati\n",
      "Hebrew\nInuktitut_(Canadian_Aboriginal_Syllabics)\nJapanese_(hiragana)\n",
      "Japanese_(katakana)\n",
      "Korean\nLatin\n",
      "Malay_(Jawi_-_Arabic)\n",
      "Mkhedruli_(Georgian)\nN_Ko\n",
      "Ojibwe_(Canadian_Aboriginal_Syllabics)\nSanskrit\n",
      "Syriac_(Estrangelo)\nTagalog\nTifinagh",
      "\n",
      "Angelic\n",
      "Atemayar_Qelisayer\n",
      "Atlantean\n",
      "Aurek-Besh\n",
      "Avesta\n",
      "Ge_ez\n",
      "Glagolitic\n",
      "Gurmukhi\n",
      "Kannada\n",
      "Keble\n",
      "Malayalam\n",
      "Manipuri\n",
      "Mongolian\n",
      "Old_Church_Slavonic_(Cyrillic)\n",
      "Oriya\n",
      "Sylheti\n",
      "Syriac_(Serto)\n",
      "Tengwar\n",
      "Tibetan\n",
      "ULOG\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "Xtrain,ytrain,ctrain=loading_data(train_folder)\n",
    "Xval,yval,cval=loading_data(val_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are around 30 training Aplhabets examples and 20 validation alphabets available for training and validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def initialize_weights(shape):\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def initialize_bias(shape):\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* To define model for one-shot learning we make use of siamese network, with 2 similar CNN layers as inputs\n",
    "* One input layer is the actual image to be compared with and other image is the image being used to compare\n",
    "* Both the results are passed on to final Sigmoid layer to better calculate the difference after the output from both layers\n",
    "* Here i's using Keras to create the model, with functional layer , to work with multiple inputs and single output\n",
    "* Because keras doesnt support layer to calculate output difference, we use custom Lambda layer to find absolute difference in output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def siamese_model(input_shape):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape, kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid', kernel_regularizer=l2(1e-3), kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    image_one = model(left_input)\n",
    "    image_two = model(right_input)\n",
    "    \n",
    "    input_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    difference = input_layer([image_one, image_two])\n",
    "    \n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(difference)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    return siamese_net\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n",
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n",
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\n",
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\n",
      "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 105, 105, 1)  0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 105, 105, 1)  0                                            \n__________________________________________________________________________________________________\nsequential_1 (Sequential)       (None, 4096)         38947648    input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n                                                                 sequential_1[2][0]               \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n==================================================================================================\nTotal params: 38,951,745\nTrainable params: 38,951,745\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = siamese_model((105, 105, 1))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n",
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n\n",
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Using binary_crossentropy since the output value is in 0 and 1\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Here we create a batch of data, which output PAIR of images which is the 2 Inputs for our model, and TARGET values which are the similarity value between those values\n",
    "* Batch is created based on the given value, 20 in our case\n",
    "* These batch of data is split into two part as per our training, \n",
    "    * 1st half of the values from batch are different and randomly selected\n",
    "    * 2nd half of the values from batch are same and randomly selected\n",
    "* Hence the TARGET value for 1st half is 0, 2nd half is 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# We will use this block only for Training our model\n",
    "def get_batch(batch_size,s=\"train\"):\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "    else:\n",
    "        X = Xval\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "    # This selects few characters in random to use in our training\n",
    "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "\n",
    "    # Create empty array for Images and Target values\n",
    "    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "    targets=np.zeros((batch_size,))\n",
    "    \n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = rng.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = rng.randint(0, n_examples)\n",
    "        \n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "        \n",
    "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "    \n",
    "    return pairs, targets\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# This returns data in pairs for validation\n",
    "def test_oneshot_preprocess(N, s=\"val\"):\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "    else:\n",
    "        X = Xval\n",
    "    n_classes, n_examples, w, h = X.shape \n",
    "    indices = rng.randint(0, n_examples,size=(N,))\n",
    "    \n",
    "    categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "    test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "    support_set = X[categories,indices,:,:]\n",
    "    support_set[0,:,:] = X[true_category,ex2]\n",
    "    support_set = support_set.reshape(N, w, h,1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image,support_set]\n",
    "\n",
    "    return pairs, targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We predict and evaluate the model by running the prediction on our validation set \n",
    "and calculate average accuracy of our model,\n",
    "* This is important way of evaluating the model on batch of 20 one-shot classification\n",
    "* As the validation happens in batches the validation set accuracy can be taken from the overall average"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def predict_oneshot(model, N, k, s = \"val\"):\n",
    "    n_correct = 0\n",
    "    for i in range(k):\n",
    "        inputs, targets = test_oneshot_preprocess(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    return percent_correct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Predefine few Hyper parameters\n",
    "n_iter = 1000 # Total iterations\n",
    "n_val = 200 # validate on this number of set\n",
    "evaluate_every = 200 # evaluate model every 100 iterations\n",
    "batch_size = 64\n",
    "N_way = 20 # to be tested on 20 way images \n",
    "best = -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Optional) Here I am loading my pre trained weights which I already trained on the dataset and saved before to avoid repeated training for the notebook,\n",
    "We can still wish to not use this and train from beginning without loading weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 105, 105, 1)  0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 105, 105, 1)  0                                            \n__________________________________________________________________________________________________\nsequential_1 (Sequential)       (None, 4096)         38947648    input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n                                                                 sequential_1[2][0]               \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n==================================================================================================\nTotal params: 38,951,745\nTrainable params: 38,951,745\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\chand\\Anaconda3\\envs\\Tensorflow1.4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.load_weights(\"D:\\MLworkspace\\OneShotLearning\\omniglot_1\\data\\pretrained_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because of the pre-trained weights the model should converge at around 5 to 6 evaluation, "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs,targets) = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n Training Loss: {0}\".format(loss)) \n",
    "        val_acc = predict_oneshot(model, N_way, n_val)\n",
    "        model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n Training Loss: 0.15595930814743042\n",
      "Current best: 76.5, previous best: -1\n",
      "\n Training Loss: 0.24491146206855774\n",
      "Current best: 78.5, previous best: 76.5\n",
      "\n Training Loss: 0.13311876356601715\n",
      "Current best: 84.0, previous best: 78.5\n",
      "\n Training Loss: 0.1227961853146553\n",
      "Current best: 86.0, previous best: 84.0\n",
      "\n Training Loss: 0.12759816646575928\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see the final accuracy achieved with our weight is around 86% on the validation set \n",
    " , While the accuracy  of Test data is around 88-90% before it stops converging\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}